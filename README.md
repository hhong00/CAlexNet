# CAlexNet

# Goal

The goal of this project was to implement a cache in a convolutional neural network (CNN) model, exploring how an image-similarity cache could improve model performance in speed or accuracy. Specifically, we built our cache on top of a modified version of AlexNet, which was famous for winning the 2012 ImageNet challenge, and tested the performance of the CNN with and without the cache. Our main goal was to potentially speed up the CNN’s evaluation time by returning cached values for similar images, without drastically sacrificing the accuracy of the original CNN at test time –– after several epochs of training. The cache is built-up during training, and used during evaluation; the cache hit algorithm identifies whether or not the current test image is similar enough to an already-processed image in the cache––based on a similarity metric of our choosing. Furthermore, the frequency of cache hits can be dependent on a threshold hyperparameter ((\tau)) of our choosing, representing the required similarity for a cache hit. Upon a cache hit, the evaluation algorithm uses this pre-trained value from the cache, simply returning the result of the most similar image in the cache. Otherwise, the model would do the forward pass on that image with the AlexNet model, as if it was a normal evaluation phase. We built our cache on top of the backend implementation of AlexNet on an existing Jupyter notebook, which allowed us to focus on the cache implementation. AlexNet performs image classification on the CIFAR-10 dataset; the notebook trains the model on 50,000 images and tests the images on 10,000 images, classifying them into 10 distinct labels.

# Design

The cache’s core structure was a hashmap (i.e. Python dictionary) storing key, value pairs. In our case, the keys were the tuple-ized image tensors themselves, which were flattened so that they could be hashed into the python dictionary. The values were 4-tuples consisting of the mean, standard deviation, label class, and output logits returned from the AlexNet model while running a forward pass on that respective image tensor key. The mean and standard deviation were essential for computing the similarity metrics, while the labels and logits were the important values to return from our cache. This cache is therefore––very verbosely––named a Mean-Std-Similarity (MSS) cache. The MSS cache would only be written to during the last epoch of training because during the last epoch, the model has the most gradient accumulations and the most accurate weights; as a result, the last epoch is optimal for populating the cache with the logits (i.e. the last epoch gives the best weights or probabilities for a given image’s classifications).
